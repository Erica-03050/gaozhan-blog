#!/usr/bin/env python3
"""
è·å–å®Œæ•´æ–‡ç« å†…å®¹è„šæœ¬
ä¸ºç°æœ‰çš„æ–‡ç« è·å–HTMLæ ¼å¼çš„å®Œæ•´å†…å®¹
"""

import requests
import json
import time
import os
from datetime import datetime

class ContentFetcher:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://www.dajiala.com"
        self.session = requests.Session()

    def get_article_html_content(self, article_url: str) -> dict:
        """è·å–æ–‡ç« çš„HTMLå†…å®¹"""
        url = f"{self.base_url}/fbmain/monitor/v3/article_html"
        params = {
            'key': self.api_key,
            'url': article_url
        }
        
        try:
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if result.get('code') == 0:
                return result.get('data', {})
            else:
                print(f"è·å–HTMLå†…å®¹å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                return {}
                
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
            return {}

    def update_articles_with_content(self, sync_file: str):
        """ä¸ºåŒæ­¥æ–‡ä»¶ä¸­çš„æ–‡ç« æ·»åŠ å®Œæ•´å†…å®¹"""
        
        print(f"ğŸ”„ å¼€å§‹ä¸ºæ–‡ç« è·å–å®Œæ•´å†…å®¹...")
        print(f"ğŸ“ è¯»å–æ–‡ä»¶: {sync_file}")
        
        # è¯»å–ç°æœ‰åŒæ­¥æ•°æ®
        with open(sync_file, 'r', encoding='utf-8') as f:
            sync_data = json.load(f)
        
        total_articles = 0
        updated_articles = 0
        total_cost = 0.0
        
        # éå†æ‰€æœ‰è´¦æˆ·çš„æ–‡ç« 
        for account_idx, account in enumerate(sync_data['account_results']):
            account_name = account['account_name']
            print(f"\nğŸ“° å¤„ç†è´¦æˆ·: {account_name}")
            
            for article_idx, article in enumerate(account['articles']):
                total_articles += 1
                article_url = article.get('original_url', '')
                article_title = article.get('title', 'N/A')
                
                if not article_url:
                    print(f"  âš ï¸ è·³è¿‡æ–‡ç« ï¼ˆæ— URLï¼‰: {article_title[:50]}...")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰å†…å®¹
                if article.get('content') and len(article['content']) > 100:
                    print(f"  âœ… å·²æœ‰å†…å®¹: {article_title[:50]}...")
                    continue
                
                print(f"  ğŸ” è·å–å†…å®¹: {article_title[:50]}...")
                
                # è·å–HTMLå†…å®¹
                html_data = self.get_article_html_content(article_url)
                
                if html_data:
                    # æ›´æ–°æ–‡ç« å†…å®¹
                    article['content'] = html_data.get('content', '')
                    article['content_html'] = html_data.get('content', '')  # ä¿ç•™HTMLæ ¼å¼
                    
                    # ç”Ÿæˆæ‘˜è¦
                    plain_text = html_data.get('content', '').replace('<', ' <').replace('>', '> ')
                    import re
                    plain_text = re.sub(r'<[^>]+>', '', plain_text)
                    article['excerpt'] = plain_text[:200].strip() + ('...' if len(plain_text) > 200 else '')
                    
                    updated_articles += 1
                    total_cost += 0.04  # HTMLå†…å®¹è·å–è´¹ç”¨
                    
                    print(f"    âœ… æˆåŠŸè·å– {len(article['content'])} å­—ç¬¦")
                else:
                    print(f"    âŒ è·å–å¤±è´¥")
                
                # é¿å…è¯·æ±‚è¿‡é¢‘
                time.sleep(1.5)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        sync_data['content_update_stats'] = {
            'total_articles': total_articles,
            'updated_articles': updated_articles,
            'update_time': datetime.now().isoformat(),
            'additional_cost': total_cost
        }
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        updated_filename = sync_file.replace('.json', '_with_content.json')
        with open(updated_filename, 'w', encoding='utf-8') as f:
            json.dump(sync_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… å†…å®¹æ›´æ–°å®Œæˆ!")
        print(f"ğŸ“Š æ€»æ–‡ç« æ•°: {total_articles}")
        print(f"ğŸ”„ æ›´æ–°æ–‡ç« æ•°: {updated_articles}")
        print(f"ğŸ’° é¢å¤–è´¹ç”¨: {total_cost:.2f}å…ƒ")
        print(f"ğŸ“ ä¿å­˜åˆ°: {updated_filename}")

def main():
    """ä¸»å‡½æ•°"""
    API_KEY = "JZLebac614e9c88d8b4"
    
    # æŸ¥æ‰¾æœ€æ–°çš„åŒæ­¥æ–‡ä»¶
    sync_files = [f for f in os.listdir('.') if f.startswith('sync_results_') and f.endswith('.json')]
    if not sync_files:
        print("âŒ æœªæ‰¾åˆ°åŒæ­¥ç»“æœæ–‡ä»¶")
        return
    
    latest_file = sorted(sync_files)[-1]
    print(f"ğŸ¯ ä½¿ç”¨æœ€æ–°åŒæ­¥æ–‡ä»¶: {latest_file}")
    
    # ç¡®è®¤æ“ä½œ
    response = input(f"\næ˜¯å¦è¦ä¸º {latest_file} ä¸­çš„æ–‡ç« è·å–å®Œæ•´å†…å®¹ï¼Ÿ(y/n): ")
    if response.lower() != 'y':
        print("æ“ä½œå·²å–æ¶ˆ")
        return
    
    # åˆ›å»ºå†…å®¹è·å–å™¨
    fetcher = ContentFetcher(API_KEY)
    
    # è·å–å†…å®¹
    fetcher.update_articles_with_content(latest_file)

if __name__ == "__main__":
    main()

